import os, requests, re, sys
from urllib.parse import urlparse
from concurrent.futures import ThreadPoolExecutor, as_completed

# --- åŸºç¡€é…ç½® ---
CURRENT_DIR = os.path.dirname(os.path.abspath(__file__))
PARENT_DIR = os.path.dirname(CURRENT_DIR)
MERGED_SOURCE = os.path.join(PARENT_DIR, "history", "merged.txt")
MANUAL_FIX = os.path.join(CURRENT_DIR, "manual_fix.txt")

TIMEOUT = 2
MAX_THREADS_CHECK = 100 # ç¬¬ä¸€é˜¶æ®µä½“æ£€å¹¶å‘
MAX_THREADS_SCAN = 40   # ç¬¬äºŒé˜¶æ®µçˆ†ç ´å¹¶å‘

def check_url(url):
    """æ£€æµ‹å•ä¸ª URL æ˜¯å¦é€šç•…"""
    try:
        # stream=True åªè¯»å¤´éƒ¨ï¼Œé€Ÿåº¦æœ€å¿«
        with requests.get(url, timeout=TIMEOUT, stream=True, headers={"User-Agent":"VLC/3.0"}) as r:
            return r.status_code == 200
    except:
        return False

def get_existing_ip_ports():
    """ä»ç°æœ‰çš„ manual_fix.txt ä¸­æå–æ‰€æœ‰ IP:ç«¯å£"""
    ip_ports = set()
    if os.path.exists(MANUAL_FIX):
        try:
            with open(MANUAL_FIX, 'r', encoding='utf-8') as f:
                content = f.read()
                # åŒ¹é…æ ¼å¼å¦‚ 123.123.123.123:808
                found = re.findall(r'(\d{1,3}\.\d{1,3}\.\d{1,3}\.\d{1,3}:\d+)', content)
                ip_ports.update(found)
        except Exception as e:
            print(f"âš ï¸ è¯»å–ç°æœ‰åº“å¤±è´¥: {e}")
    return ip_ports

def main():
    if not os.path.exists(MERGED_SOURCE):
        print(f"âŒ é”™è¯¯ï¼šæ‰¾ä¸åˆ°æºæ–‡ä»¶ {MERGED_SOURCE}", flush=True)
        return

    # 0. é¢„è¯»å–ç°æœ‰åº“ï¼Œé˜²æ­¢é‡å¤è¿½åŠ 
    existing_set = get_existing_ip_ports()
    print(f"ğŸ“‘ ç°æœ‰åº“æ£€æµ‹ï¼šå·²å­˜åœ¨ {len(existing_set)} ä¸ªå”¯ä¸€ç½‘æ®µã€‚", flush=True)

    # 1. è§£æåŸå§‹ç½‘æ®µ
    ip_groups = {}
    with open(MERGED_SOURCE, 'r', encoding='utf-8', errors='ignore') as f:
        for line in f:
            line = line.strip()
            if "," not in line or "http" not in line: continue
            url = line.split(',', 1)[1].strip()
            ip_port = urlparse(url).netloc
            if ip_port:
                if ip_port not in ip_groups: ip_groups[ip_port] = []
                ip_groups[ip_port].append(line)

    print(f"ğŸ“– åŸºå› åº“è§£æå®Œæˆï¼Œå…± {len(ip_groups)} ä¸ªåŸå§‹ç½‘æ®µã€‚", flush=True)
    
    final_results = []
    found_ips = set() # æœ¬æ¬¡ä»»åŠ¡ä¸­æ–°å‘ç°çš„ IP
    to_rescue = []

    # --- é˜¶æ®µ 1ï¼šå…ˆæµ‹å­˜æ´» (å…¨é‡ä½“æ£€) ---
    print(f"\nğŸ“¡ é˜¶æ®µ 1ï¼šå…¨é‡ä½“æ£€å¼€å§‹ (å¹¶å‘:{MAX_THREADS_CHECK})...", flush=True)
    with ThreadPoolExecutor(max_workers=MAX_THREADS_CHECK) as executor:
        future_to_ip = {executor.submit(check_url, data[0].split(',')[1]): ip for ip, data in ip_groups.items()}
        for future in as_completed(future_to_ip):
            ip_port = future_to_ip[future]
            if future.result():
                # è¿‡æ»¤ï¼šå¦‚æœåº“é‡Œå·²ç»æœ‰äº†ï¼Œå°±ä¸å†å¤„ç†
                if ip_port not in existing_set and ip_port not in found_ips:
                    found_ips.add(ip_port)
                    print(f"  âœ… [æ–°å‘ç°-å­˜æ´»] {ip_port}", flush=True)
                    block = f"{ip_port},#genre#\n" + "\n".join(ip_groups[ip_port]) + "\n\n"
                    final_results.append(block)
                # å¦‚æœå·²åœ¨åº“ä¸­ï¼Œé»˜é»˜è·³è¿‡
            else:
                to_rescue.append(ip_port)

    # --- é˜¶æ®µ 2ï¼šåªå¯¹å¤±æ•ˆ IP è¿›è¡Œçˆ†ç ´ ---
    if to_rescue:
        print(f"\nğŸš€ é˜¶æ®µ 2ï¼šå¼€å§‹åœ°æ¯¯å¼çˆ†ç ´å¤±æ•ˆç½‘æ®µ (å¾…å¤„ç†:{len(to_rescue)})...", flush=True)
        to_rescue.sort()
        for idx, base_ip_port in enumerate(to_rescue):
            ip_parts = base_ip_port.split(':')
            if len(ip_parts) != 2: continue
            ip, port = ip_parts
            if not re.match(r'^\d+\.\d+\.\d+\.\d+$', ip): continue
            
            prefix = '.'.join(ip.split('.')[:-1])
            channels = ip_groups[base_ip_port]
            path = channels[0].split(',')[1].split(base_ip_port)[-1]
            
            test_tasks = {f"http://{prefix}.{i}:{port}{path}": f"{prefix}.{i}:{port}" for i in range(1, 256)}
            
            with ThreadPoolExecutor(max_workers=MAX_THREADS_SCAN) as executor:
                future_to_url = {executor.submit(check_url, url): target_ip for url, target_ip in test_tasks.items()}
                for future in as_completed(future_to_url):
                    target_ip = future_to_url[future]
                    if future.result():
                        # è¿‡æ»¤ï¼šåº“é‡Œæ²¡æœ‰ ä¸” æœ¬æ¬¡ä¹Ÿæ²¡å‘ç°è¿‡
                        if target_ip not in existing_set and target_ip not in found_ips:
                            found_ips.add(target_ip)
                            print(f"  âœ¨ [å‘½ä¸­æ–°æº!!] -> {target_ip}", flush=True)
                            new_block = f"{target_ip},#genre#\n"
                            for ch in channels:
                                name, old_url = ch.split(',', 1)
                                new_url = old_url.replace(base_ip_port, target_ip)
                                new_block += f"{name},{new_url}\n"
                            final_results.append(new_block + "\n")

    # 3. æœ€ç»ˆè¿½åŠ å†™å…¥ manual_fix.txt
    if final_results:
        print(f"\nğŸ’¾ å‡†å¤‡å†™å…¥ï¼šæœ¬æ¬¡å…±å‘ç° {len(final_results)} ä¸ªæ–°ç½‘æ®µã€‚", flush=True)
        try:
            with open(MANUAL_FIX, 'a', encoding='utf-8') as f:
                # æ£€æŸ¥æ–‡ä»¶æœ«å°¾æ˜¯å¦æœ‰æ¢è¡Œ
                if os.path.exists(MANUAL_FIX) and os.path.getsize(MANUAL_FIX) > 0:
                    f.write("\n\n")
                f.writelines(final_results)
            print(f"ğŸ‰ ä»»åŠ¡å®Œæˆï¼æ–°å†…å®¹å·²è¿½åŠ è‡³ {MANUAL_FIX}", flush=True)
        except Exception as e:
            print(f"âŒ å†™å…¥å¤±è´¥: {e}")
    else:
        print("\nğŸ“­ æœ¬æ¬¡æ‰«ææœªå‘ç°åº“ä»¥å¤–çš„æ–°ç½‘æ®µã€‚", flush=True)

if __name__ == "__main__":
    main()

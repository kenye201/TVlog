import os, requests, concurrent.futures, re
from urllib.parse import urlparse

# --- é…ç½®åŒº ---
CURRENT_DIR = os.path.dirname(os.path.abspath(__file__))
PARENT_DIR = os.path.dirname(CURRENT_DIR)
MERGED_SOURCE = os.path.join(PARENT_DIR, "history", "merged.txt")
MANUAL_FIX = os.path.join(CURRENT_DIR, "manual_fix.txt")

TIMEOUT = 3
MAX_WORKERS_IP = 40  # æå– IP çš„å¹¶å‘
MAX_WORKERS_C = 60   # C æ®µçˆ†ç ´çš„å¹¶å‘

def extract_ip_port(url):
    try:
        parsed = urlparse(url)
        if parsed.netloc: return parsed.netloc
    except: return None
    return None

def check_url(url):
    """æ£€æµ‹å•ä¸ª URL æ˜¯å¦å­˜æ´»"""
    try:
        r = requests.get(url, timeout=TIMEOUT, stream=True, headers={"User-Agent":"VLC/3.0"})
        return r.status_code == 200
    except:
        return False

def scan_c_segment(base_ip_port, channel_list):
    """
    å¯¹å¤±æ•ˆ IP è¿›è¡Œ C æ®µçˆ†ç ´ (1-255)
    è¿”å›ç¬¬ä¸€ä¸ªæ‰«åˆ°çš„æ´» IP å—å†…å®¹
    """
    ip, port = base_ip_port.split(':')
    prefix = '.'.join(ip.split('.')[:-1])
    
    # æ„é€ æ¢æµ‹ä»»åŠ¡ï¼šæ‰«æè¯¥ C æ®µæ‰€æœ‰ 255 ä¸ªåœ°å€
    test_tasks = []
    for i in range(1, 256):
        target_ip = f"{prefix}.{i}:{port}"
        # æ‹¿ç¬¬ä¸€ä¸ªé¢‘é“çš„è·¯å¾„æ¥æµ‹è¯•
        path = channel_list[0].split(',')[1].split(base_ip_port)[-1]
        test_url = f"http://{target_ip}{path}"
        test_tasks.append((target_ip, test_url))

    with concurrent.futures.ThreadPoolExecutor(max_workers=MAX_WORKERS_C) as executor:
        future_to_ip = {executor.submit(check_url, url): t_ip for t_ip, url in test_tasks}
        for future in concurrent.futures.as_completed(future_to_ip):
            target_ip = future_to_ip[future]
            if future.result():
                print(f"âœ¨ C æ®µçˆ†ç ´æˆåŠŸ: {base_ip_port} -> {target_ip}")
                # æ„é€ æ–°çš„é¢‘é“å—å†…å®¹
                new_block = f"{target_ip},#genre#\n"
                for ch in channel_list:
                    name, old_url = ch.split(',', 1)
                    new_url = old_url.replace(base_ip_port, target_ip)
                    new_block += f"{name},{new_url}\n"
                return new_block + "\n"
    return None

def main():
    if not os.path.exists(MERGED_SOURCE):
        print("âŒ æœªæ‰¾åˆ° history/merged.txt")
        return

    # 1. è§£æå½’ç±»
    ip_groups = {}
    with open(MERGED_SOURCE, 'r', encoding='utf-8', errors='ignore') as f:
        for line in f:
            line = line.strip()
            if "," not in line or "http" not in line: continue
            parts = line.split(',', 1)
            ip_port = extract_ip_port(parts[1].strip())
            if ip_port:
                if ip_port not in ip_groups: ip_groups[ip_port] = []
                ip_groups[ip_port].append(line)

    print(f"ğŸ“– åŸºå› åº“è§£æå®Œæˆï¼Œå…± {len(ip_groups)} ä¸ªåŸå§‹ç½‘æ®µã€‚")
    
    final_results = []

    # 2. ä¸²è¡Œå¤„ç†æ¯ä¸ªç½‘æ®µï¼ˆå†…éƒ¨ä½¿ç”¨å¹¶å‘ï¼‰
    for idx, (ip_port, channels) in enumerate(ip_groups.items()):
        print(f"[{idx+1}/{len(ip_groups)}] æ­£åœ¨å¤„ç†: {ip_port}")
        
        # å…ˆæµ‹åŸå§‹ IP
        test_url = channels[0].split(',')[1]
        if check_url(test_url):
            print(f"âœ… åŸå§‹ IP å­˜æ´»: {ip_port}")
            block = f"{ip_port},#genre#\n" + "\n".join(channels) + "\n\n"
            final_results.append(block)
        else:
            # åŸå§‹ IP ä¸é€šï¼Œç«‹å³çˆ†ç ´ C æ®µ
            print(f"ğŸš€ åŸå§‹ IP å¤±æ•ˆï¼Œå¼€å§‹ C æ®µçˆ†ç ´...")
            rescued_block = scan_c_segment(ip_port, channels)
            if rescued_block:
                final_results.append(rescued_block)
            else:
                print(f"ğŸ’€ è¯¥ç½‘æ®µå½»åº•å¤±æ•ˆï¼Œå·²æ”¾å¼ƒã€‚")

    # 3. è¦†ç›–å†™å…¥ manual_fix.txt
    if final_results:
        with open(MANUAL_FIX, 'w', encoding='utf-8') as f:
            f.writelines(final_results)
        print(f"ğŸ‰ ä»»åŠ¡å®Œæˆï¼å…±å¯¼å‡º {len(final_results)} ä¸ªæ´»ç½‘æ®µè‡³ manual_fix.txt")
    else:
        print("âš ï¸ æœªå‘ç°ä»»ä½•å­˜æ´»æˆ–å¯ä¿®å¤çš„ç½‘æ®µã€‚")

if __name__ == "__main__":
    main()

import os, requests, concurrent.futures, re
from urllib.parse import urlparse

# --- è·¯å¾„é…ç½® ---
CURRENT_DIR = os.path.dirname(os.path.abspath(__file__))
PARENT_DIR = os.path.dirname(CURRENT_DIR)
MERGED_SOURCE = os.path.join(PARENT_DIR, "history", "merged.txt")
MANUAL_FIX = os.path.join(CURRENT_DIR, "manual_fix.txt")

TIMEOUT = 4
MAX_WORKERS = 50

def extract_ip_port(url):
    """ä» URL ä¸­æå– Host:Port"""
    try:
        parsed = urlparse(url)
        if parsed.netloc:
            return parsed.netloc
    except:
        return None
    return None

def load_fix_ips():
    """è¯»å–è¡¥ä¸åº“ç°æœ‰çš„æ‰€æœ‰ IP"""
    ips = set()
    if os.path.exists(MANUAL_FIX):
        with open(MANUAL_FIX, 'r', encoding='utf-8', errors='ignore') as f:
            # åŒ¹é…æ‰€æœ‰å½¢å¦‚ 1.2.3.4:80 çš„å­—ç¬¦ä¸²
            found = re.findall(r'([\w\.\-]+:\d+)', f.read())
            for item in found:
                ips.add(item.strip())
    return ips

def main():
    existing_ips = load_fix_ips()
    # ç»“æ„: { "122.114.131.154:4060": [ "CCTV1,url1", "CCTV2,url2" ] }
    ip_groups = {} 

    print(f"ğŸ“– æ­£åœ¨è§£ææ··åˆæºæ–‡ä»¶: {MERGED_SOURCE}")
    if not os.path.exists(MERGED_SOURCE):
        print("âŒ é”™è¯¯ï¼šæ‰¾ä¸åˆ°æ–‡ä»¶")
        return

    # --- 1. æ‰«ææå–å¹¶æŒ‰ IP èšåˆ ---
    with open(MERGED_SOURCE, 'r', encoding='utf-8', errors='ignore') as f:
        for line in f:
            line = line.strip()
            if "," not in line or "http" not in line:
                continue
            
            parts = line.split(',', 1)
            name = parts[0].strip()
            url = parts[1].strip()
            
            ip_port = extract_ip_port(url)
            if ip_port and ip_port not in existing_ips:
                if ip_port not in ip_groups:
                    ip_groups[ip_port] = []
                # å­˜å…¥é¢‘é“åå’Œå®Œæ•´ URL
                ip_groups[ip_port].append(f"{name},{url}")

    if not ip_groups:
        print("âœ… å¤§åº“ä¸­æ‰€æœ‰ IP å·²å­˜åœ¨äºè¡¥ä¸åº“æˆ–æœªå‘ç°æœ‰æ•ˆ URLã€‚")
        return

    print(f"ğŸ“¡ æå–åˆ° {len(ip_groups)} ä¸ªå…¨æ–°ç½‘æ®µï¼Œå¼€å§‹æ¢æµ‹å­˜æ´»...")

    # --- 2. å¹¶å‘æ¢æµ‹ ---
    newly_found = []
    
    def check_worker(ip):
        try:
            # æŠ½å–è¯¥ IP ä¸‹çš„ç¬¬ä¸€ä¸ªé¢‘é“é“¾æ¥è¿›è¡Œæµ‹è¯•
            test_url = ip_groups[ip][0].split(',')[1]
            r = requests.get(test_url, timeout=TIMEOUT, stream=True, headers={"User-Agent":"VLC/3.0"})
            return ip, r.status_code == 200
        except:
            return ip, False

    

    with concurrent.futures.ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:
        future_to_ip = {executor.submit(check_worker, ip): ip for ip in ip_groups}
        for future in concurrent.futures.as_completed(future_to_ip):
            ip, ok = future.result()
            if ok:
                print(f"ğŸŒŸ [æŒ–åˆ°æ–°çŸ¿] {ip} ({len(ip_groups[ip])} é¢‘é“)")
                # æŒ‰ç…§ä½ å–œæ¬¢çš„ IP åˆ†ç»„æ ¼å¼æ„å»ºå—
                block = f"{ip},#genre#\n"
                block += "\n".join(ip_groups[ip])
                block += "\n\n"
                newly_found.append(block)

    # --- 3. è¿½åŠ å†™å…¥ ---
    if newly_found:
        with open(MANUAL_FIX, 'a', encoding='utf-8') as f:
            f.writelines(newly_found)
        print(f"ğŸš€ è¿½åŠ å®Œæˆï¼æœ¬æ¬¡å‘ç° {len(newly_found)} ä¸ªæ–°æ´»ç½‘æ®µå¹¶å·²æ ¼å¼åŒ–å­˜å…¥è¡¥ä¸åº“ã€‚")
    else:
        print("â›ˆï¸ æ¢æµ‹ç»“æŸï¼Œæ²¡å‘ç°èƒ½è¿é€šçš„æ–°æºã€‚")

if __name__ == "__main__":
    main()

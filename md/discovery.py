import os, requests, re, sys
from urllib.parse import urlparse
from concurrent.futures import ThreadPoolExecutor, as_completed

# --- åŸºç¡€é…ç½® ---
CURRENT_DIR = os.path.dirname(os.path.abspath(__file__))
PARENT_DIR = os.path.dirname(CURRENT_DIR)
MERGED_SOURCE = os.path.join(PARENT_DIR, "history", "merged.txt")
MANUAL_FIX = os.path.join(CURRENT_DIR, "manual_fix.txt")

TIMEOUT = 2
MAX_THREADS_CHECK = 100
MAX_THREADS_SCAN = 60 # ç¨å¾®æé«˜çˆ†ç ´æ•ˆç‡

def check_url(url):
    try:
        with requests.get(url, timeout=TIMEOUT, stream=True, headers={"User-Agent":"VLC/3.0"}) as r:
            return r.status_code == 200
    except:
        return False

def get_existing_ip_ports():
    """ä»ç°æœ‰çš„ manual_fix.txt ä¸­æå–æ‰€æœ‰ IP:ç«¯å£ï¼Œç¡®ä¿å½»åº•å»é‡"""
    ip_ports = set()
    if os.path.exists(MANUAL_FIX):
        try:
            with open(MANUAL_FIX, 'r', encoding='utf-8') as f:
                content = f.read()
                # å¢å¼ºæ­£åˆ™ï¼šç¡®ä¿åŒ¹é…æ›´ç²¾å‡†
                found = re.findall(r'(\d{1,3}\.\d{1,3}\.\d{1,3}\.\d{1,3}:\d+)', content)
                ip_ports.update(found)
        except Exception as e:
            print(f"âš ï¸ è¯»å–ç°æœ‰åº“å¤±è´¥: {e}")
    return ip_ports

def main():
    if not os.path.exists(MERGED_SOURCE):
        print(f"âŒ é”™è¯¯ï¼šæ‰¾ä¸åˆ°æºæ–‡ä»¶ {MERGED_SOURCE}")
        return

    # 1. åˆå§‹åŒ–å·²å­˜åœ¨é›†åˆï¼ˆè¿™æ˜¯å»é‡çš„å…³é”®ï¼‰
    existing_set = get_existing_ip_ports()
    print(f"ğŸ“‘ ç°æœ‰åº“æ£€æµ‹ï¼šå·²å­˜åœ¨ {len(existing_set)} ä¸ªå”¯ä¸€ç½‘æ®µã€‚")

    # 2. è§£æåŸå§‹ç½‘æ®µ
    ip_groups = {}
    with open(MERGED_SOURCE, 'r', encoding='utf-8', errors='ignore') as f:
        for line in f:
            line = line.strip()
            if "," not in line or "http" not in line: continue
            parts = line.split(',', 1)
            url = parts[1].strip()
            ip_port = urlparse(url).netloc
            if ip_port:
                if ip_port not in ip_groups: ip_groups[ip_port] = []
                ip_groups[ip_port].append(line)

    final_results_dict = {} # ä½¿ç”¨å­—å…¸å­˜å‚¨ {ip_port: block_text} å®ç°è‡ªåŠ¨å»é‡
    to_rescue = []

    # --- é˜¶æ®µ 1ï¼šå…¨é‡ä½“æ£€ ---
    print(f"\nğŸ“¡ é˜¶æ®µ 1ï¼šå…¨é‡ä½“æ£€å¼€å§‹...")
    with ThreadPoolExecutor(max_workers=MAX_THREADS_CHECK) as executor:
        future_to_ip = {executor.submit(check_url, data[0].split(',')[1]): ip for ip, data in ip_groups.items()}
        for future in as_completed(future_to_ip):
            ip_port = future_to_ip[future]
            if future.result():
                # æŸ¥é‡ï¼šåº“é‡Œæ²¡æœ‰ ä¸” è¿˜æ²¡è¢«æœ¬æ¬¡æ‰«æè®°å…¥
                if ip_port not in existing_set:
                    print(f"  âœ… [æ–°å‘ç°-å­˜æ´»] {ip_port}")
                    block = f"{ip_port},#genre#\n" + "\n".join(ip_groups[ip_port]) + "\n\n"
                    final_results_dict[ip_port] = block
                    existing_set.add(ip_port) # å®æ—¶æ ‡è®°å·²å­˜åœ¨ï¼Œé˜²æ­¢é˜¶æ®µ2é‡å¤å‘½ä¸­
            else:
                to_rescue.append(ip_port)

    # --- é˜¶æ®µ 2ï¼šå¤±æ•ˆ IP çˆ†ç ´ ---
    if to_rescue:
        print(f"\nğŸš€ é˜¶æ®µ 2ï¼šçˆ†ç ´å¤±æ•ˆç½‘æ®µ (æ•°é‡:{len(to_rescue)})...")
        to_rescue.sort()
        for base_ip_port in to_rescue:
            ip_parts = base_ip_port.split(':')
            if len(ip_parts) != 2: continue
            ip, port = ip_parts
            prefix = '.'.join(ip.split('.')[:-1])
            channels = ip_groups[base_ip_port]
            
            # è·å–è¯·æ±‚è·¯å¾„
            sample_url = channels[0].split(',')[1]
            path = sample_url.split(base_ip_port)[-1]
            
            test_tasks = {f"http://{prefix}.{i}:{port}{path}": f"{prefix}.{i}:{port}" for i in range(1, 256)}
            
            with ThreadPoolExecutor(max_workers=MAX_THREADS_SCAN) as executor:
                future_to_url = {executor.submit(check_url, url): target_ip for url, target_ip in test_tasks.items()}
                for future in as_completed(future_to_url):
                    target_ip = future_to_url[future]
                    if future.result():
                        # æ ¸å¿ƒæŸ¥é‡ï¼šé˜²æ­¢çˆ†ç ´å‡ºçš„ IP ä¸ åº“å†… æˆ– é˜¶æ®µ1 å†²çª
                        if target_ip not in existing_set:
                            print(f"  âœ¨ [å‘½ä¸­æ–°æº!!] -> {target_ip}")
                            new_block = f"{target_ip},#genre#\n"
                            for ch in channels:
                                name, old_url = ch.split(',', 1)
                                new_url = old_url.replace(base_ip_port, target_ip)
                                new_block += f"{name},{new_url}\n"
                            final_results_dict[target_ip] = new_block + "\n"
                            existing_set.add(target_ip) # å†æ¬¡å®æ—¶æ ‡è®°

    # 3. å†™å…¥æ–‡ä»¶
    if final_results_dict:
        # æœ€åçš„é˜²å¾¡ï¼šå†æå–ä¸€éåº“æ–‡ä»¶ IP å†æ¬¡å¯¹æ¯”ï¼Œé˜²æ­¢å¤šå®ä¾‹è¿è¡Œå†²çª
        re_check_set = get_existing_ip_ports()
        unique_final = [text for ip, text in final_results_dict.items() if ip not in re_check_set]

        if unique_final:
            print(f"\nğŸ’¾ å†™å…¥ä¸­ï¼šè¿‡æ»¤é‡å¤åï¼Œæœ¬æ¬¡å®é™…æ–°å¢ {len(unique_final)} ä¸ªç½‘æ®µã€‚")
            with open(MANUAL_FIX, 'a', encoding='utf-8') as f:
                content = "".join(unique_final)
                if os.path.exists(MANUAL_FIX) and os.path.getsize(MANUAL_FIX) > 0:
                    # ç¡®ä¿æ–‡ä»¶æœ«å°¾æœ‰ä¸”åªæœ‰ä¸€ä¸ªç©ºè¡Œå†è¿½åŠ 
                    f.write("\n\n")
                f.write(content.strip() + "\n")
            print(f"ğŸ‰ ä»»åŠ¡å®Œæˆï¼")
        else:
            print("\nğŸ“­ è¿‡æ»¤åæ— æ–°ç½‘æ®µå¯å†™å…¥ã€‚")
    else:
        print("\nğŸ“­ æœ¬æ¬¡æ‰«ææœªå‘ç°æ–°ç½‘æ®µã€‚")

if __name__ == "__main__":
    main()

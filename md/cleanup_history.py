import hashlib
from pathlib import Path
import os

# --- é…ç½® ---
HISTORY_FOLDER = Path("history")
# è¦ä¿ç•™çš„æ–‡ä»¶çš„ç±»å‹ã€‚merged.* æ–‡ä»¶é€šå¸¸éœ€è¦ä¿ç•™ã€‚
FILE_PATTERNS = ["*.m3u", "*.txt"]
# ä¿ç•™ç­–ç•¥: 'earliest' (ä¿ç•™æ—¶é—´æˆ³æœ€æ—©çš„æ–‡ä»¶), 'latest' (ä¿ç•™æ—¶é—´æˆ³æœ€æ–°çš„æ–‡ä»¶)
RETENTION_POLICY = 'earliest' 
# ---------------

def get_file_hash(file_path):
    """è®¡ç®—æ–‡ä»¶çš„ SHA256 å“ˆå¸Œå€¼ï¼Œç”¨äºå†…å®¹æ¯”è¾ƒã€‚"""
    hasher = hashlib.sha256()
    # ä»¥äºŒè¿›åˆ¶æ¨¡å¼åˆ†å—è¯»å–æ–‡ä»¶ï¼Œå¤„ç†å¤§æ–‡ä»¶
    with open(file_path, 'rb') as f:
        while chunk := f.read(4096):
            hasher.update(chunk)
    return hasher.hexdigest()

def cleanup_duplicate_files():
    print(f"ğŸ§¹ å¼€å§‹æ¸…ç† {HISTORY_FOLDER} æ–‡ä»¶å¤¹ä¸­çš„é‡å¤æ–‡ä»¶...")
    
    # ç»“æ„: { æ–‡ä»¶ç±»å‹: { å†…å®¹å“ˆå¸Œ: [æ–‡ä»¶è·¯å¾„åˆ—è¡¨ (æŒ‰æ—¶é—´æˆ³æ’åº)] } }
    duplicates = {}
    
    for pattern in FILE_PATTERNS:
        # åªå¤„ç†å¸¦æ—¶é—´æˆ³çš„å¤‡ä»½æ–‡ä»¶ï¼Œå¿½ç•¥ 'merged' æ–‡ä»¶
        files_to_check = [f for f in HISTORY_FOLDER.glob(pattern) 
                          if not f.name.startswith("merged.")]
        
        # æŒ‰æ–‡ä»¶åï¼ˆå³æ—¶é—´æˆ³ï¼‰æ’åºæ–‡ä»¶ï¼Œç¡®ä¿ä¿ç•™ç­–ç•¥çš„æ­£ç¡®æ€§
        # æ–‡ä»¶åå¦‚ logo12081157.m3uï¼Œsorted() é»˜è®¤æŒ‰å­—æ¯é¡ºåºæ’ï¼Œæ—¶é—´æˆ³è¶Šå°è¶Šé å‰
        files_to_check.sort(key=lambda f: f.name) 
        
        duplicates[pattern] = {}
        
        for file_path in files_to_check:
            try:
                file_hash = get_file_hash(file_path)
                
                if file_hash not in duplicates[pattern]:
                    duplicates[pattern][file_hash] = []
                    
                duplicates[pattern][file_hash].append(file_path)
                
            except Exception as e:
                print(f"è­¦å‘Šï¼šæ— æ³•è¯»å–æ–‡ä»¶ {file_path}: {e}")
    
    total_removed = 0
    
    # éå†æ‰€æœ‰æ–‡ä»¶ç±»å‹å’Œå“ˆå¸Œå€¼
    for pattern, hash_groups in duplicates.items():
        for file_hash, file_list in hash_groups.items():
            
            # å¦‚æœåˆ—è¡¨é•¿åº¦å¤§äº 1ï¼Œåˆ™å­˜åœ¨é‡å¤é¡¹
            if len(file_list) > 1:
                
                # ä¿ç•™çš„æ–‡ä»¶ç´¢å¼•
                if RETENTION_POLICY == 'latest':
                    # æ–‡ä»¶åˆ—è¡¨å·²æŒ‰æ—¶é—´æˆ³ä»å°åˆ°å¤§æ’åºï¼Œä¿ç•™æœ€åä¸€ä¸ª
                    file_to_keep = file_list[-1]
                    files_to_delete = file_list[:-1]
                else: # 'earliest'
                    # ä¿ç•™ç¬¬ä¸€ä¸ª
                    file_to_keep = file_list[0]
                    files_to_delete = file_list[1:]
                
                print(f"\nå‘ç° {len(file_list)} ä¸ªå†…å®¹ç›¸åŒçš„é‡å¤æ–‡ä»¶ ({pattern}, å“ˆå¸Œ: {file_hash[:8]}...)")
                print(f"âœ… ä¿ç•™æ–‡ä»¶: {file_to_keep.name}")
                
                # åˆ é™¤é‡å¤æ–‡ä»¶
                for f_path in files_to_delete:
                    try:
                        os.remove(f_path)
                        print(f"âŒ åˆ é™¤é‡å¤é¡¹: {f_path.name}")
                        total_removed += 1
                    except Exception as e:
                        print(f"è­¦å‘Šï¼šåˆ é™¤æ–‡ä»¶ {f_path} å¤±è´¥: {e}")
                        
    print(f"\nâœ… æ¸…ç†å®Œæˆï¼å…±åˆ é™¤ {total_removed} ä¸ªé‡å¤çš„å¤‡ä»½æ–‡ä»¶ã€‚")

if __name__ == "__main__":
    if not HISTORY_FOLDER.exists():
        print(f"é”™è¯¯ï¼šæ–‡ä»¶å¤¹ {HISTORY_FOLDER} ä¸å­˜åœ¨ï¼Œè·³è¿‡æ¸…ç†ã€‚")
    else:
        cleanup_duplicate_files()

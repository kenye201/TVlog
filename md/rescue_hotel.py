import os, requests, re, concurrent.futures
from urllib.parse import urlparse

# --- è·¯å¾„é”å®š ---
CURRENT_DIR = os.path.dirname(os.path.abspath(__file__))
# æ˜ç¡®æŒ‡å‘ md ç›®å½•ä¸‹çš„ä¸´æ—¶æ–‡ä»¶
INPUT_DEAD = os.path.join(CURRENT_DIR, "dead_tasks.txt")
OUTPUT_RESCUED = os.path.join(CURRENT_DIR, "rescued_temp.txt")

TIMEOUT = 2
MAX_WORKERS = 60

def is_valid_ip(ip_str):
    pattern = r'^\d{1,3}\.\d{1,3}\.\d{1,3}\.\d{1,3}:[0-9]+$'
    return bool(re.match(pattern, ip_str))

def check_url(url):
    try:
        r = requests.get(url, timeout=TIMEOUT, stream=True)
        return True if r.status_code == 200 else False
    except: return False

def main():
    # æ‰“å°è·¯å¾„è°ƒè¯•ä¿¡æ¯
    print(f"ğŸš‘ æ£€æŸ¥æŠ¢æ•‘æ¸…å•: {INPUT_DEAD}", flush=True)
    if not os.path.exists(INPUT_DEAD):
        print("âš ï¸ æœªå‘ç°å¾…æŠ¢æ•‘ä»»åŠ¡æ–‡ä»¶ï¼Œè·³è¿‡ã€‚")
        return

    with open(INPUT_DEAD, 'r', encoding='utf-8') as f:
        blocks = [b.strip() for b in f.read().split('\n\n') if b.strip()]
    
    # è¿‡æ»¤æ ¼å¼
    valid_blocks = []
    for b in blocks:
        header = b.split('\n')[0].split(',')[0]
        if is_valid_ip(header):
            valid_blocks.append(b)
    
    if not valid_blocks:
        print("ğŸ“Š å¾…æŠ¢æ•‘æ¸…å•ä¸­æ²¡æœ‰ç¬¦åˆæ ¼å¼çš„ IP æ®µã€‚")
        return

    print(f"ğŸš‘ å‡†å¤‡æŠ¢æ•‘ {len(valid_blocks)} ä¸ªç½‘æ®µ...", flush=True)

    with open(OUTPUT_RESCUED, 'w', encoding='utf-8') as f_out:
        for idx, block in enumerate(valid_blocks, 1):
            lines = block.split('\n')
            old_ip = lines[0].split(',')[0]
            try:
                ip_part, port = old_ip.split(':')
                prefix = ".".join(ip_part.split('.')[:3])
                path = urlparse(lines[1].split(',')[1]).path
                
                print(f"[{idx}/{len(valid_blocks)}] ğŸ” çˆ†ç ´ C æ®µ: {prefix}.x:{port}", flush=True)
                
                found = False
                tasks = [f"http://{prefix}.{i}:{port}{path}" for i in range(1, 255)]
                
                with concurrent.futures.ThreadPoolExecutor(max_workers=MAX_WORKERS) as exe:
                    future_to_url = {exe.submit(check_url, u): u for u in tasks}
                    for fut in concurrent.futures.as_completed(future_to_url):
                        if fut.result():
                            new_host = urlparse(future_to_url[fut]).netloc
                            print(f"   âœ¨ [æ•‘å›æˆåŠŸ] {old_ip} -> {new_host}", flush=True)
                            f_out.write(f"{new_host},#genre#\n")
                            for l in lines[1:]:
                                name, url = l.split(',', 1)
                                f_out.write(f"{name},http://{new_host}{urlparse(url).path}\n")
                            f_out.write("\n")
                            found = True
                            exe.shutdown(wait=False, cancel_futures=True)
                            break
                if not found:
                    print(f"   âŒ [å¤±è´¥]", flush=True)
            except Exception as e:
                print(f"   âš ï¸ é”™è¯¯: {e}", flush=True)
                continue

    print("\nğŸ æŠ¢æ•‘é˜¶æ®µå…¨éƒ¨ç»“æŸã€‚", flush=True)

if __name__ == "__main__":
    main()
